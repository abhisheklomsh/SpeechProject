{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6166abf",
   "metadata": {},
   "source": [
    "### We have cleaned the data and stored in appropriate folders and now we move on to modelling component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063dd24",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71888311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f5a31",
   "metadata": {},
   "source": [
    "## Clean and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93586a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data(test_size=0.4):\n",
    "    x, y = [], []\n",
    "    for file in os.listdir(os.path.join('data', 'Parsed_Capuchinbird_Clips')):\n",
    "        # Load audio file\n",
    "        signal, sr = librosa.load(os.path.join('data', 'Parsed_Capuchinbird_Clips', file), sr=None, duration=5)\n",
    "        # Extract features\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=30)\n",
    "        chroma = librosa.feature.chroma_stft(y=signal, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=signal, sr=sr,n_bands=1)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=signal, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=signal, sr=sr)\n",
    "        features = np.concatenate([mfccs.mean(axis=1), chroma.mean(axis=1), contrast.mean(axis=1), spectral_centroids.mean(axis=1),\n",
    "                               spectral_rolloff.mean(axis=1),\n",
    "                               mel_spectrogram.mean(axis=1)])\n",
    "        # Append features and labels\n",
    "        x.append(features)\n",
    "        y.append(1)\n",
    "    for file in os.listdir(os.path.join('data', 'Parsed_Not_Capuchinbird_Clips')):\n",
    "        # Load audio file\n",
    "        signal, sr = librosa.load(os.path.join('data', 'Parsed_Not_Capuchinbird_Clips', file), sr=None, duration=5)\n",
    "        # Extract features\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=30)\n",
    "        chroma = librosa.feature.chroma_stft(y=signal, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=signal, sr=sr,n_bands=1)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=signal, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=signal, sr=sr)\n",
    "        features = np.concatenate([mfccs.mean(axis=1), chroma.mean(axis=1), contrast.mean(axis=1), spectral_centroids.mean(axis=1),\n",
    "                               spectral_rolloff.mean(axis=1),\n",
    "                               mel_spectrogram.mean(axis=1)])\n",
    "        # Append features and labels\n",
    "        x.append(features)\n",
    "        y.append(0)\n",
    "    # Convert to numpy arrays\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)\n",
    "    # Split into train and test sets - with class balancing using stratify technique\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,stratify=y, random_state=42)\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054baaf",
   "metadata": {},
   "source": [
    "## Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc3e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/librosa/core/pitch.py:102: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier accuracy: 0.9907407407407407\n",
      "Confusion matrix:\n",
      " [[235   2]\n",
      " [  1  86]]\n",
      "SVC accuracy: 0.9783950617283951\n",
      "Confusion matrix:\n",
      " [[236   1]\n",
      " [  6  81]]\n",
      "KNeighborsClassifier accuracy: 0.9753086419753086\n",
      "Confusion matrix:\n",
      " [[235   2]\n",
      " [  6  81]]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "#Grid Search \n",
    "param_grid = {'n_estimators': [50, 100, 200, 500],\n",
    "              'max_depth': [10, 20, 30, None]}\n",
    "\n",
    "def train_evaluate_models(x_train, x_test, y_train, y_test):\n",
    "    models = [RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "              SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0),\n",
    "              KNeighborsClassifier(n_neighbors=5)]\n",
    "    for model in models:\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(type(model).__name__, \"accuracy:\", acc)\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_data()\n",
    "train_evaluate_models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40d092dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging classifier accuracy: 0.9938271604938271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       237\n",
      "           1       0.99      0.99      0.99        87\n",
      "\n",
      "    accuracy                           0.99       324\n",
      "   macro avg       0.99      0.99      0.99       324\n",
      "weighted avg       0.99      0.99      0.99       324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost classifier accuracy: 0.9783950617283951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       237\n",
      "           1       0.98      0.94      0.96        87\n",
      "\n",
      "    accuracy                           0.98       324\n",
      "   macro avg       0.98      0.97      0.97       324\n",
      "weighted avg       0.98      0.98      0.98       324\n",
      "\n",
      "Ensemble accuracy: 0.9814814814814815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       237\n",
      "           1       0.99      0.94      0.96        87\n",
      "\n",
      "    accuracy                           0.98       324\n",
      "   macro avg       0.98      0.97      0.98       324\n",
      "weighted avg       0.98      0.98      0.98       324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_344403/2923319603.py:20: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  y_pred_ensemble = np.array([mode([y_pred_bagging[i], y_pred_adaboost[i]])[0][0] for i in range(len(y_pred_bagging))])\n"
     ]
    }
   ],
   "source": [
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                 n_estimators=50,\n",
    "                                 max_samples=0.5,\n",
    "                                 max_features=0.5)\n",
    "bagging_clf.fit(x_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(x_test)\n",
    "print(\"Bagging classifier accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
    "print(classification_report(y_test, y_pred_bagging, target_names=[\"0\",\"1\"]))\n",
    "\n",
    "# Train an AdaBoost classifier\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                                  n_estimators=200,\n",
    "                                  learning_rate=0.1)\n",
    "adaboost_clf.fit(x_train, y_train)\n",
    "y_pred_adaboost = adaboost_clf.predict(x_test)\n",
    "print(\"AdaBoost classifier accuracy:\", accuracy_score(y_test, y_pred_adaboost))\n",
    "print(classification_report(y_test, y_pred_adaboost, target_names=[\"0\",\"1\"]))\n",
    "\n",
    "# Combine the predictions of the Bagging and AdaBoost classifiers using majority voting\n",
    "y_pred_ensemble = np.array([mode([y_pred_bagging[i], y_pred_adaboost[i]])[0][0] for i in range(len(y_pred_bagging))])\n",
    "print(\"Ensemble accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e850959",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22229b",
   "metadata": {},
   "source": [
    "#### We can see the models are giving highly accurate results on cleaned data with very limited False Positives and False Negatives in the Confusion Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
